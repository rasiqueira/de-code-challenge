# Code Challenge: Data Engineer

## Contexto

Somos uma empresa que est√° transformando dados em insights para melhorar a experi√™ncia do cliente e otimizar opera√ß√µes. Estamos construindo uma plataforma robusta que integra dados de diversas fontes e permite a an√°lise eficiente e confi√°vel desses dados. Sua tarefa √© implementar um pipeline de ETL que processe dados e os torne acess√≠veis para consulta, an√°lise e visualiza√ß√£o.

---

## Fontes de Dados

Voc√™ receber√° dois conjuntos de dados **aprimorados** com estruturas mais realistas e cen√°rios complexos:

- **Transa√ß√µes**: Representa opera√ß√µes entre a empresa e seus clientes, incluindo valores transacionados, condi√ß√µes, metadados ricos e dados de qualidade variada.
- **Pagamentos**: Representa os pagamentos realizados por clientes, incluindo cen√°rios complexos como pagamentos parciais, antecipados, atrasados e m√∫ltiplos por transa√ß√£o.

### üìä **Dados Aprimorados (Vers√£o 2.0)**

Os dados foram significativamente melhorados para tornar o desafio mais realista:

- [transactions.json](transactions.json) - **500 transa√ß√µes** com estrutura rica
- [payments.json](payments.json) - **602 pagamentos** com cen√°rios complexos
- [data_structure_example.json](data_structure_example.json) - Documenta√ß√£o da estrutura

### üéØ **Melhorias Implementadas**

#### **1. Dados Mais Realistas**
- ‚úÖ IDs √∫nicos n√£o sequenciais (UUIDs)
- ‚úÖ Valores variados e distribui√ß√£o realista (70% baixo/m√©dio, 20% alto, 10% muito alto)
- ‚úÖ M√∫ltiplas moedas (BRL 85%, USD 10%, EUR 5%)
- ‚úÖ Datas de vencimento vari√°veis (15-90 dias)
- ‚úÖ Metadados ricos (categoria, canal, desconto, parcelamento)

#### **2. Cen√°rios de Inadimpl√™ncia Complexos**
- ‚úÖ **Sem pagamento** (~10% das transa√ß√µes)
- ‚úÖ **Pagamentos parciais m√∫ltiplos** (~15% com 2-5 parcelas)
- ‚úÖ **Pagamentos atrasados** (~20% com atrasos de 1-120 dias)
- ‚úÖ **Pagamentos antecipados** (~8%)
- ‚úÖ **Pagamentos a maior** (~2%)
- ‚úÖ **M√∫ltiplas transa√ß√µes por cliente** (~30% dos clientes)

#### **3. Estrutura de Dados Rica**
```json
{
  "transaction_id": "txn_a1b2c3d4e5f6",
  "customer_id": "cust_xyz789ab",
  "amount": 1250.50,
  "currency": "BRL",
  "transaction_date": "2024-01-15",
  "due_date": "2024-02-15",
  "payment_terms": "30_days",
  "status": "pending",
  "metadata": {
    "product_category": "electronics",
    "sales_rep": "rep_001",
    "channel": "online",
    "discount_applied": 10,
    "installments": 3
  }
}
```

#### **4. Desafios de Qualidade de Dados**
- ‚úÖ ~3% dos registros t√™m problemas intencionais
- ‚úÖ Valores nulos, tipos incorretos, datas inv√°lidas
- ‚úÖ Testa robustez do pipeline ETL

---

## Defini√ß√µes

- **Transa√ß√£o**: Uma opera√ß√£o comercial que gera obriga√ß√µes financeiras, como uma compra ou contrato de servi√ßo.
- **Pagamento**: Um registro de que uma obriga√ß√£o financeira foi quitada.

---

## Desafio ETL

Como uma empresa orientada por dados, √© crucial termos uma infraestrutura que permita o armazenamento, processamento e an√°lise dos dados de maneira eficiente e escal√°vel.

Sua tarefa √© construir um pipeline de ETL (Extract, Transform, Load) que:

1. **Extraia** os dados dos arquivos fornecidos, tratando-os de forma adequada;
2. **Transforme** os dados aplicando regras de neg√≥cio, agrega√ß√µes e tratamento adequado (limpeza, convers√µes e enriquecimento);
3. **Carregue** os dados em um formato estruturado, armazenado de forma que possa ser consultado via SQL.

A solu√ß√£o deve ser escal√°vel e preparada para lidar com um aumento significativo no volume de dados no futuro.

### Requisitos

- Implementar o pipeline de ETL.
- Os dados processados devem ser armazenados de forma que possam ser consultados via SQL (por exemplo, atrav√©s de tabelas em um data lake ou banco de dados compat√≠vel).
- A solu√ß√£o deve tratar os dados de forma segura, garantindo que n√£o haja perda de informa√ß√µes.
- A implementa√ß√£o deve contemplar o armazenamento de resultados utilizando arquivos Parquet, particionados por um crit√©rio que identifique a inadimpl√™ncia (por exemplo, `inadimplente = true` ou `false`).

### Diferencial

- Implementar o pipeline de ETL utilizando **Apache Spark** (Scala ou Python).
- Emule uma solu√ß√£o de **streaming** utilizando os arquivos fornecidos como fonte de dados.

---

## Governan√ßa e Privacidade

### Desafio

Al√©m de processar e armazenar os dados, √© fundamental que a solu√ß√£o implemente pol√≠ticas de governan√ßa e privacidade:

- **Governan√ßa**:  
  - Descreva e, se poss√≠vel, implemente uma solu√ß√£o de metadados que controle quem pode acessar quais dados.  
  - Explique como voc√™ faria o gerenciamento de permiss√µes e o controle de acesso aos dados processados.

- **Privacidade**:  
  - Mostre como garantir que os dados sens√≠veis sejam protegidos, incluindo abordagens para anonimiza√ß√£o e/ou criptografia dos dados.
  - Caso opte por implementar, integre essa solu√ß√£o ao pipeline ETL.

### Diferencial

- Implemente no pipeline uma etapa voltada para a governan√ßa e privacidade, acompanhada de uma documenta√ß√£o que explique as escolhas e estrat√©gias.

---

## Regras de Neg√≥cio

### Contexto

Queremos analisar o comportamento dos clientes em rela√ß√£o a pagamentos atrasados. Para isso, estabelecemos dois crit√©rios para classificar um cliente como inadimplente:

- **Crit√©rio 1**: O cliente atrasou um pagamento em mais de 30 dias, ap√≥s um per√≠odo de 3 meses.
- **Crit√©rio 2**: O cliente teve algum atraso superior a 15 dias em um per√≠odo de 6 meses.

### Sua Tarefa

- Implemente uma l√≥gica que avalie os clientes com base nesses crit√©rios para um per√≠odo de 6 meses.  
- Forne√ßa os seguintes resultados:
  - A porcentagem de clientes inadimplentes.
  - A lista de clientes considerados inadimplentes, detalhando os respectivos pagamentos que violaram os crit√©rios.
- Armazene os resultados como arquivos Parquet, particionados por um campo que indique se o cliente √© inadimplente (`inadimplente = true` ou `false`).

### üéØ **Novos Desafios com Dados Aprimorados**

Os dados melhorados introduzem cen√°rios mais complexos que seu pipeline deve tratar:

#### **Cen√°rios de Pagamento Complexos**
- **Pagamentos Parciais**: Como tratar transa√ß√µes com m√∫ltiplos pagamentos parciais?
- **Pagamentos Atrasados Variados**: Atrasos de 1 a 120 dias - como calcular inadimpl√™ncia?
- **M√∫ltiplas Transa√ß√µes por Cliente**: Como agregar o comportamento de pagamento?
- **Pagamentos Antecipados**: Devem ser considerados na an√°lise de risco?

#### **Qualidade de Dados**
- **Dados Inconsistentes**: ~3% dos registros t√™m problemas intencionais
- **Valores Nulos**: Como tratar transa√ß√µes sem valor?
- **Datas Inv√°lidas**: Como validar e corrigir datas?
- **Tipos Incorretos**: Como converter dados com tipos errados?

#### **An√°lises Adicionais Poss√≠veis**
- **Por M√©todo de Pagamento**: PIX vs Cart√£o vs Boleto - qual tem maior inadimpl√™ncia?
- **Por Categoria de Produto**: Electronics vs Clothing - padr√µes diferentes?
- **Por Canal de Venda**: Online vs Loja f√≠sica - comportamentos distintos?
- **Por Valor da Transa√ß√£o**: Transa√ß√µes altas vs baixas - correla√ß√£o com inadimpl√™ncia?

### Requisitos

- A solu√ß√£o deve ser confi√°vel e preparada para escalabilidade em volumes maiores de dados.

### Diferencial

- Implemente o pipeline de ETL utilizando **Apache Spark** (Scala ou Python).

---

## Crit√©rios de Avalia√ß√£o

- **Efici√™ncia**:  
  - Avaliaremos o tempo de execu√ß√£o do pipeline e o uso de recursos, principalmente em opera√ß√µes distribu√≠das com Apache Spark.
  
- **Clareza do C√≥digo**:  
  - O c√≥digo deve ser claro, com nomea√ß√£o adequada de vari√°veis, fun√ß√µes e m√≥dulos.
  
- **Documenta√ß√£o**:  
  - A presen√ßa de um README.md que detalhe as instru√ß√µes de configura√ß√£o, instala√ß√£o, execu√ß√£o e explica√ß√£o das decis√µes de design.  
  - Uso de docstrings (seguindo o PEP 257) e coment√°rios esclarecedores sobre as escolhas t√©cnicas.

- **Escalabilidade**:  
  - A solu√ß√£o deve ser capaz de lidar com um aumento no volume de dados e demonstrar boas pr√°ticas para opera√ß√µes distribu√≠das.

- **Testes**:  
  - A inclus√£o de testes unit√°rios (ou de integra√ß√£o) para validar a funcionalidade do pipeline e as regras de neg√≥cio ser√° considerada um diferencial.

---

## Entrega

- **Reposit√≥rio Git**:  
  - A solu√ß√£o deve ser entregue atrav√©s de um reposit√≥rio Git. Fa√ßa uma c√≥pia do reposit√≥rio e envie do link do seu reposit√≥rio com a solu√ß√£o.
  
- **Prazo**:  
  - O prazo para a entrega da solu√ß√£o √© de 7 dias a partir do recebimento do desafio.
  
- **Instru√ß√µes de Execu√ß√£o**:  
  - Inclua um arquivo `README.md` atualizado com instru√ß√µes claras sobre como configurar o ambiente, instalar as depend√™ncias e executar a solu√ß√£o (incluindo exemplos de comandos, como `spark-submit spark_pipeline.py` e instru√ß√µes de agendamento ou execu√ß√£o automatizada se implementadas).
